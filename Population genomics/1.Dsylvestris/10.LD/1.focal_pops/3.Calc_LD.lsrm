#!/bin/bash

#SBATCH --job-name=ngsLD
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --mem-per-cpu=2000
#SBATCH --tmp=5000
#SBATCH --time=24:00:00
#SBATCH --output=./logs/ngsLD.%A.%a.log
#SBATCH --error=./errs/ngsLD.%A.%a.err

if [ ! -e logs ]  ; then mkdir logs ; fi
if [ ! -e errs ]  ; then mkdir errs ; fi

# This command calculates LD with ngsLD


#! Number of nodes and tasks per node allocated by SLURM (do not change):
numnodes=$SLURM_JOB_NUM_NODES
numtasks=$SLURM_NTASKS
mpi_tasks_per_node=$(echo "$SLURM_TASKS_PER_NODE" | sed -e  's/^\([0-9][0-9]*\).*$/\1/')
mem_gb=$(( $SLURM_MEM_PER_NODE / 1000 ))

# Load modules
source /cluster/project/gdc/shared/stack/GDCstack.sh
module load ngsld/1.2.0

#! Work directory (i.e. where the job will run):
workdir="$SLURM_SUBMIT_DIR"

#! Are you using OpenMP (NB this is unrelated to OpenMPI)? If so increase this safe value to no more than 76:
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
#! Number of MPI tasks to be started by the application per node and in total (do not change):
np=$[${numnodes}*${mpi_tasks_per_node}]
#! The following variables define a sensible pinning strategy for Intel MPI tasks -
#! this should be suitable for both pure MPI and hybrid MPI/OpenMP jobs:
export I_MPI_PIN_DOMAIN=omp:compact # Domains are $OMP_NUM_THREADS cores in size
export I_MPI_PIN_ORDER=scatter # Adjacent domains have minimal sharing of caches/sockets

###############################################################
#### RUN ###


# Print job info
JOBID=$SLURM_JOB_ID
echo -e "JobID: $JOBID\n======"
echo "Time: `date`"
echo "Running on master node: `hostname`"
echo "Current directory: `pwd`"
if [ "$SLURM_JOB_NODELIST" ]; then
        echo -e "\nNodes allocated:\n================"
fi
echo -e "\nnumtasks=$numtasks, numnodes=$numnodes, mpi_tasks_per_node=$mpi_tasks_per_node (OMP_NUM_THREADS=$OMP_NUM_THREADS)"
echo -e "\nExecuting command:\n==================\n$CMD\n"



prefix="GL_95gwas_CEN_region_w100kbFlanks_pVal12maf20depth2"
N_IND=$(cat bam_95gwas.filelist | wc -l)
N_SITES=$(cat ${prefix}.pos | wc -l)
/cluster/work/gdc/shared/p936/DSYL_ms_revisions/ngsLD/ngsLD --n_threads $SLURM_CPUS_PER_TASK --verbose 1 --n_ind $N_IND --n_sites $N_SITES --geno ${prefix}.GLF.gz --probs --pos ${prefix}.pos --max_kb_dist 0 --min_maf 0.2 --extend_out | sort -k 1,1Vr -k 2,2V > ${prefix}.ld



###############################################################
##Get a summary of the job
myjobs -j ${SLURM_JOB_ID}

####job controlling, grep "Job:" *.out
echo "Job: ${SLURM_JOB_ID}_${SLURM_ARRAY_TASK_ID} successfully finished $(date)"
# happy end
exit 0
################################################################
